---
title: "SC19022: the Homework of Statistic Computing"
author: "SC19022"
output: 
  rmarkdown::html_vignette:
    toc: true
vignette: >
  %\VignetteIndexEntry{SC19022: the Homework of Statistic Computing}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}

smart: FALSE
---
# Introduction
__SC19022__ is a package which include my all homework solutions for the course: _Statistic Computing_. This package can separate for two section. The first section is the _final homework_, it contains two function _kernden2d_pr.R_ and _kernden2d_ss.R_ , you can use they to estimate the density function for two dimensional. In this section, _rwmC.cpp_ also exits, this cppfuntion is a random walk metropolis sampler.The second section is _Routine homework_, most of the exercise from books: _Statistic Computing with R_ and _Advanced R_, some interestiong additional exercise are also presented.


# Final Homework
Probility density function is the fundamental conditinal for most statistic problems. However, in nonparametric status, we konw nothing about the information of real distribution. Fortunately, kernal density estimation can help us find more information, kernel function and bandwidth influence the estimated results significantly. The solution for __Multivariate KDE__ normally can be implemented by two kind of kernel:

1. __Product kernel__
2. __spherical-symmetric kernel__

the package support the below two functions to estimate in two dimensional.

## kernden2d_pr.R

product kernel:$K(u) = K_1(u_1) \cdot K_2(u_2)$

where $K$ is the kernel function. then we have:
\begin{align}
\hat f_h(x) = \frac{1}{n}\sum^2_{i=1}\frac{1}{h_1h_2}K(\frac{x_1-X_{i1}}{h_1},\frac{x_1-X_{i1}}{h_1})\\=\frac{1}{n}\sum^2_{i=1}\frac{1}{h_1h_2}K(\frac{x_1-X_{i1}}{h_1})\cdot K(\frac{x_2-X_{i2}}{h_2})
\end{align}

```{r}
kernden2d_pr <- function(x, y, hx , hy, kernel) {
  # kernel function
  if (kernel=="gaussian") 
  {kern <- function(x) dnorm(x)}
  else if (kernel=="uniform")
  {kern <- function(x) 0.5 * (x>=-1 & x<=1)}
  else if (kernel=="epanechnikov")
  {kern <- function(x) (3/4)*(1-x^2)*(x>=-1 & x<=1)}
  else if (kernel=="biweight")     
  {kern <- function(x) (15/16)*(1-x^2)^2*(x>=-1 & x<=1)}
  else if (kernel=="triweight")    
  {kern <- function(x) (35/32)*(1-x^2)^3*(x>=-1 & x<=1)}
 
  # select the value of x, y corresponding to f.hat
  xx <- seq( min(x)-3*hx, max(x)+3*hx, l = 30)
  yy <- seq( min(y)-3*hy, max(y)+3*hy, l = 30)
  
  ux <- outer(xx, x, "-") / hx
  uy <- outer(yy, y, "-") / hy
  
  # kernel weights
  Kx <- kern(ux)
  Ky <- kern(uy)
  
  fx <- (Kx %*% t(Ky)) / (hx*hy*length(x))
  
  list(x=xx, y=yy, f.hat = fx)
}
```

_kerden2d_pr.R_ support kernel function: gaussian, uniform, epanechnikov, biweight, triweight.

we use the data set "faithful" to presente the results.
```{r}
data("faithful")
res <- kernden2d_pr(faithful$eruptions, faithful$waiting, 0.23, 33, kernel="gaussian")
persp(res$x, res$y, res$f.hat, theta = -30, phi = 30, 
      col = "lightblue", xlab = "eruptions", zlab = "kde",
      ylab = "waiting", main = "product kernel")
```


## kernden2d_ss.R
spherical-symmetric kernel:
$\mathcal{K}({u})=\frac{K(\|{u}\|)}{\int_{\mathbb{R}^{d}} K(\|{u}\|) \mathrm d {u}}$

where$\|{u}\|=\sqrt{{u}^{\prime} {u}}$
```{r}
kernden2d_ss <- function(x, y, hx, hy, kernel) {
  # kernel function
  if (kernel=="gaussian") 
  {kern <- function(x,y) exp(-(x^2+y^2)/2) / (2*pi)}
  else if (kernel=="epanechnikov")
  {kern <- function(x,y) 9/16*(1-(x^2+y^2)) * (x^2+y^2 <= 1)}
  
  # select the value of x, y corresponding to f.hat
  xx <- seq( min(x)-3*hx, max(x)+3*hx, l = 30)
  yy <- seq( min(y)-3*hy, max(y)+3*hy, l = 30)
  
  ux <- outer(xx, x, "-") / hx
  uy <- outer(yy, y, "-") / hy
  
  m <- matrix(0, nrow = 30, ncol = 30)
  z <- numeric( length(x) )
  for (i in seq_along(xx)) {
    for (j in seq_along(xx)) {
      for (d in seq_along(x)) {
        z[d] <- kern( ux[i,d], uy[j,d])
      }
      m[i,j] <- mean(z) / (hx*hy)
    }
  }
  
  list(x = xx, y= yy, f.hat = m)
}
```

_kernden2d_ss.R_ support kernel function: gaussian, epanechnikov.

we also test with the "faithful" dataset.
```{r}
data("faithful")
res <- kernden2d_ss(faithful$eruptions, faithful$waiting, 0.23, 33, kernel="gaussian")
persp(res$x, res$y, res$f.hat, theta = -30, phi = 30, 
      col = "red", xlab = "eruptions", zlab = "kde",
      ylab = "waiting", main = "product kernel")
```

## rwmC.cpp
_rwmC.cpp_ is a random walk Metropolis sampler for the standard Laplace distribution using RCPP.
```{r, eval=FALSE}
NumericVector rwmC(double sigma, double x0, int N) {
  NumericVector x(N);
  x[0] = x0;
  NumericVector u = as<NumericVector>(runif(N));
  
  for(int i = 1; i < N; i++){
    double y = as<double>(rnorm(1, x[i-1], sigma));
    if (u[i] <= (exp(-abs(y))/2) / (exp(-abs(x[i-1])) / 2)){
      x[i] = y;
    }else{
      x[i] = x[i-1];
    }
  }
  return(x);
}
```



# Routine Homework
## Homework 1
### Question
1. Suppose r.v. X={6,9,8,6,7}.Correspondingly,the pdf is p={0.2,0.15,0.15,0.3,0.2}, find the expectation and variance.

2. Printing a figure about a random data.

3. Printing a table about a random data.

### Answer
1.
```{r}
x<-c(16,19,18,16,20)
p<-c(0.2,0.15,0.15,0.3,0.2)
EX<-sum(x*p)
VarX<-sum(x^2*p)-EX^2
```
The $EX$ is `r EX` and the $VarX$ is `r VarX`.

2.
```{r}
height<-c(160,175,172)
weight<-c(55,66,62)
plot(weight,height)
```

3.
```{r}

n1<-c("jack","peter","fisher")
n2<-c("age","height/cm","weight/kg")
a<-c(18,17,20,160,175,172,55,66,62)
m<-matrix(a, byrow=F, nrow=3,dimnames = list(n1,n2))
knitr::kable(m,format = "markdown")

```





## Homework 2
### Question  
3.4 The Rayleigh density is$$f(x)=\frac{x}{\sigma^2}e^\frac{-x^2}{2\sigma^2},\quad x\geq0,\sigma>0.$$Develop an algorithm to generate random samples from a Rayleigh$(σ)$ distribution.
Generate Rayleigh$(σ)$ samples for several choices of $σ > 0$ and check
that the mode of the generated samples is close to the theoretical mode $σ$
(check the histogram).

3.11 Generate a random sample of size 1000 from a normal location mixture. The components of the mixture have $N(0, 1)$ and $N(3, 1)$ distributions with mixing probabilities $p_1$ and $p_2 = 1 − p_1$. Graph the histogram of the sample with density superimposed, for $p_1 = 0.75$. Repeat with different values for $p_1$ and observe whether the empirical distribution of the mixture appears to be bimodal. Make a conjecture about the values of $p_1$ that produce bimodal mixtures.  

3.18 Write a function to generate a random sample from a $W_d(Σ, n)$ (Wishart) distribution for $n > d+ 1 ≥ 1$, based on Bartlett’s decomposition.  

### Answer 
3.4
```{r}

set.seed(100)
n<-1000; sigma<-c(0.25,1,9,25);
#different value of σ
u<-matrix(NA,nrow = 4,ncol = 1000)
y<-runif(n)
#since the CDF F(x) obey to U[0,1]

for (i in 1:4) {
  u[i,]<-sigma[i]*(-2*log(1-y))^(0.5)
  #the inverse function of F(u)
}
for (i in 1:4) { 
  xlength<-c(2,6,50,150);interval<-c(.01,.05,.3,.8)
  x<-seq(0,xlength[i],interval[i])
  hist(u[i,],breaks = x,freq = FALSE,main= c("σ=",paste(sigma[i])),sub= expression(f(x)==frac(x,sigma^2)*e^-frac(x^2,2*sigma^2)),xlab=NULL,col = "aquamarine",border = "aquamarine2")
  lines(x,(x/sigma[i]^2)*exp(-x^2/(2*sigma[i]^2)),col ="blue")
}
```
  
3.11
```{r}
set.seed(100)
n<-1000;
x1<-rnorm(n,0,1);x2<-rnorm(n,3,1)
z<-matrix(NA,nrow = 9,ncol = n)
for (i in 1:9) {
  k<-seq(0,1,.125)
  p1<-sample(c(0,1),size = n,replace = TRUE,prob = c(1-k[i],k[i]))
  z[i,]<-p1*x1+(1-p1)*x2#generate the random number from mixture distribution
  d<-seq(-5,8,.1)
  hist(z[i,],breaks = d,prob=TRUE,col = "skyblue",border = "skyblue",main= c("p1=",paste(k[i])),xlab = NULL)
  lines(d,(1/sqrt(2*pi))*(k[i]*exp(-(d^2)/2)+(1-k[i])*exp(-((d-3)^2)/2)),col="blueviolet")#the pdf of mixture distribution.
}
```
  
  By the results, we can guess that $p_1=0.5$ will produce the biomodal.  
  
3.18
```{r}
wishart<-function(n,d,sigma){
  L<-t(chol(sigma))#cholesky decomposition
  T<-matrix(0,d,d)
  for (i in 1:d) {
    T[i,i]<-sqrt(rchisq(1,n-i+1))
    j<-1
    while(i>j){
      T[i,j]<-rnorm(1,0,1)
      j<-j+1
    }
  }
  y<-L%*%T%*%t(T)%*%t(L)#the expression of r.v.
 y
}

#example
sigma<-matrix(rbind(c(5,2,1),c(2,5,2),c(1,2,4)),3,3)
x<-wishart(5,3,sigma)
x
```
This function get a random vector during once operation. In ordering to get the suitable random vector, we still need to repeat this function some times. So, it is not perfect.

## Homework 3
### 5.1 Question:
Compute a Monte Carlo estimate of$$\int_0^{\frac{\pi}{3}}sint\mathrm{d}t$$and compare your estimate with the exact value of the integral.  

### Answer:  
```{r}
# We know that X~U(a,b), so to find the integration of sin(x) in [a,b] equals to the expectation of (b-a)*sin(x)
set.seed(100)
n<-1e5;a<-0;b<-pi/3
x<-runif(n,a,b)
mc.val<-mean((b-a)*sin(x))
ex.val<--cos(b)+cos(a)
print(c(mc.val,ex.val))
```
By using monte carlo integration, the estimate is `r mc.val`, and the real integration is `r ex.val`.  

### 5.10 Question:
Use Monte Carlo integration with antithetic variables to estimate$$\int^1_0\frac{e^{-x}}{1+x^2}\mathrm{d}x$$and find the approximate reduction in variance as a percentage of the variance
without variance reduction.  

### Answer:
```{r}
set.seed(100)
m<-1e4
#simple monte carlo integration
x<-runif(m)
g1<-exp(-x)/(1+x^2)
mc1<-mean(g1)
#antithetic variables
u<-runif(m/2);v<-1-u
g2<-(exp(-u)/(1+(u)^2)+exp(-v)/(1+(v)^2))/2
mc2<-mean(g2)
#reduction in variance as a percentage
perc<-paste(round(100*(var(g1)-var(g2))/var(g1),2),"%")
print(c(mc1,mc2,perc))
```
The estimate by using simple monte carlo integration is `r mc1`, when using antithetic variables method, the estimate is `r mc2`. Reducion in variance is `r perc`.  

### 5.15 Question:
Obtain the stratified importance sampling estimate in Example 5.13 and compare
it with the result of Example 5.10.  

### Answer:  
```{r}
n<-10000
esti<-sd<-numeric(5)
g<-function(x){
  exp(-x-log(1+x^2))
}
for (i in 1:5) {
    x<-runif(n/5,(i-1)/5,i/5)
    fg<-g(x)/(exp(-x)/(1-exp(-1)))
    esti[i]<-mean(fg)
    sd[i]<-sd(fg)
  }
estimates<-mean(esti)
var<-mean(sd)
print(c(estimates,var))
```  
By the stratified importance sampling estimate, the integration is `r estimates`, it is less than example 5.10.



## Homework 4
### 6.5 Question:  
Suppose a 95% symmetric t-interval is applied to estimate a mean, but the
sample data are non-normal. Then the probability that the confidence interval
covers the mean is not necessarily equal to $0.95$. Use a Monte Carlo experiment
to estimate the coverage probability of the t-interval for random samples of
$χ^2(2)$ data with sample size $n = 20$. Compare your t-interval results with the
simulation results in Example 6.4. (The t-interval should be more robust to
departures from normality than the interval for variance.)

### Answer:
```{r}
# we use UCL to express the upper confidence limit
# LCL expresses the lower confidence limit.
# n is the sample size.
# m is the the numbers of replicates.
# 1-alpha is the confidence level.
# prob is the probability of confidence interval cover
# mean between normal sample and chisquare sample.

n     <- 20
m     <- 1000
alpha <- .05
prob  <- numeric(2)
LCL.chisq <- UCL.chisq <-
LCL.norm <- UCL.norm <- numeric(m)

f.u <- function(x){
  mean(x) + qt(1-alpha/2, df=n-1) * sqrt(var(x)/n)
}
f.l <- function(x){
  mean(x) - qt(1-alpha/2, df=n-1) * sqrt(var(x)/n)
}

for (i in 1:m) {
  x1 <- rchisq(n, df = 2)
  UCL.chisq[i] <- f.u(x1)
  LCL.chisq[i] <- f.l(x1)
  
  x2 <- rnorm(n, mean = 2, sd =2)
  UCL.norm[i] <- f.u(x2)
  LCL.norm[i] <- f.l(x2)
}
prob[1] <- mean(LCL.chisq < 2 & 2 < UCL.chisq)
prob[2] <- mean(LCL.norm < 2 & 2 < UCL.norm)
print(prob)
```  
By the results, we can see t-interval covers mean changes smally when the sample data are non-normal. Recalling example 6.4, the t-interval is more robust to non-normal than the interval for variance.   

### 6.6 Question:
Estimate the $0.025$, $0.05$, $0.95$, and $0.975$ quantiles of the skewness $\sqrt{b1}$ under
normality by a Monte Carlo experiment. Compute the standard error of the
estimates from $Var(\hat x_q) = \frac{q(1-q)}{nf(x_q)^2}$ ,using the normal approximation for the density (with
exact variance formula). Compare the estimated quantiles with the quantiles
of the large sample approximation $\sqrt{b_1}≈ N(0, 6/n)$.

### Answer:
```{r}
# m is the number of replicates
# n is the sample size of random variable from normal.
# k is the number of quantiles
# we define "quant" function to calculate the quantile
# of skewness.
# "se" is the function to calculate the stand error.

m <- 1000
n <- 100
k <- 100
y <- numeric(k)

quant <- function(q){
  y <- numeric(k)
  for (i in 1:k) {
    SK <- replicate(n, expr = {
      x <- rnorm(m, mean = 0, sd = 1)
      mean(((x-mean(x))/sd(x))^3)
    })
    y[i] <- as.numeric(quantile(SK, q))
  }
  mean(y)
}

se <- function(q){
  sqrt(q*(1-q) / n*(dnorm(0,1))^2)
}

rname <- c("large.q","est.q","se")
cname <- c("q=0.025", "q=0.05", "q=0.95", "q=0.975")
results <- matrix(NA, 3, 4, dimnames = list(rname,cname))
results[1, 1:4] <- c(qnorm(c(.025, .05, .95, .975), 0, 6/n))
results[2, 1:4] <- c(quant(.025), quant(.05), quant(.95), quant(.975))
results[3, 1:4] <- c(se(.025), se(.05), se(.95), se(.975))
print(results)
```  
The results shows that estimated quantiles approach to the quantiles of large sample approxinmation.



## Homework 5
### 6.7 Question:
Estimate the power of the skewness test of normality against symmetric
$Beta(α, α)$ distributions and comment on the results. Are the results different
for heavy-tailed symmetric alternatives such as $t(ν)$?

### Answer:
```{r}
skew <- function(x) {
  # caculates the sample skewness coefficient.
  y <- mean(x)
  return( mean((x - y)^3) / mean((x - y)^2)^1.5 )
}

beta  <- seq(1, 100, 5)  # this is parameter of beta distribution.
m     <- 10000           # this is number of replicates.
n     <- 50              # this is the sample size.
alpha <- .05             # this is the significance level.
cv    <- qnorm(.975, 0, sqrt(6*(n-2) / ((n+1)*(n+3))))
                         # this is the critical value in test.
pwr   <- numeric( length(beta) )
                         # pwr is the value of value.
for (i in 1:length(beta)) {
  reject <- replicate(m, expr = {
    # simulate x from beta(beta, beta).
    x <- rbeta(n, beta[i], beta[i])
    as.integer( abs(skew(x)>cv) )
  })
  pwr[i] <- mean(reject)
}
plot(beta, pwr, type = "b", xlab = "beta of beta(beta, beta)", 
     ylab = "power", ylim = c(0, .05))
se <- sqrt(pwr * (1-pwr) / m) #add standard errors
lines(beta, pwr+se, lty = 3)
lines(beta, pwr-se, lty = 3)

# for t(theta)
theta <- c(seq(1, 100,5))
pwr   <- numeric( length(theta) )
for (i in 1:length(theta)) {
  reject <- replicate(m, expr = {
    # simulate x from t distribution.
    x <- rt(n, df = theta)
    as.integer( abs(skew(x)>cv) )
  })
  pwr[i] <- mean(reject)
}
plot(theta, pwr, type = "b", xlab = "degree of freedom in t", 
     ylab = "power", ylim = c(0, .5))
abline(h = .05, lty = 2)
se <- sqrt(pwr * (1-pwr) / m) #add standard errors
lines(theta, pwr+se, lty = 3)
lines(theta, pwr-se, lty = 3)
```

> The first picture describes that the symmetric beta distribution possesses normality whatever the parameter. On the contrary, in the second piture, the estimated power is greater than alpha. This means t distribution do not have normality.

### 6.A Question:
Use Monte Carlo simulation to investigate whether the empirical Type I error
rate of the t-test is approximately equal to the nominal significance level
$α$, when the sampled population is non-normal. The t-test is robust to mild
departures from normality. Discuss the simulation results for the cases where
the sampled population is (i) $χ^2(1)$, (ii) $Uniform(0,2)$, and (iii) $Exponential(1)$. In each case, test $H_0: μ = μ_0$ vs $H_1: μ \neq μ_0$, where $μ_0$ is the mean of $χ^2(1), Uniform(0,2), Exponential(1)$, respectively.

### Answer:
```{r}
alpha <- .05  # this is the significance level.
n <- 100      # this is the sample size.
m <- 10000    # this is the number of replicates.
p <- matrix(0, nrow = 3, ncol = m)
m.data <- matrix(0, nrow = 3, ncol = n)
for (i in 1:m) {      # m replicates
  m.data[1, ] <- rchisq(n, df = 1)
  m.data[2, ] <- runif(n, min = 0, max = 2)
  m.data[3, ] <- rexp(n, rate = 1)
  for (j in 1:3) {    # t.test for chisquare, uniform, 
    # and exponential distributions.
    ttest <- t.test(m.data[j, ], alternative = "two.sided", mu = 1)
    p[j,i] <- as.numeric(ttest$p.value < alpha)
  }
}
t1e <- apply(p, 1, mean)
print(t(t1e))
```
> This shows empirical typer I error is least when the sample distribution is $U(0,2)$, and it is approximately equal to the nominal significance level 0.5. the $χ^2(1)$ sample distribution have the greatest empirical typer I error.  

### Discussion
If we obtain the powers for two methods under a particular simulation setting with 10,000 experiments: say, 0.651 for one method and 0.676 for another method. Can we say the powers are different at 0.05 level?

### Answer:

> We let $p_1$ and $p_2$ are powers of the same test. For example , in this problem, $p_1 = 0.651$ and $p_2 = 0.676$ 
So, we can chose paired-t test. $\mu_1, \mu_2$ is the expectation of $p_1, p_2$ respectively, and $\mu = \mu_1 - \mu_2$.  
To test whether the mean of $p_1$ equals to $p_2$ ,the test follows below:  
$$H_0: \mu = 0\quad vs \quad H_1: \mu \neq 0$$
obviously, we need to repeat n times of simulating experiments by two methods.  
This means that there are n couples of sample ($p_{1_i}, p_{2_i}, i= 1, 2,..., n$)  
and t-statistic is $$T = \frac{\bar d}{s_d / \sqrt{n}}$$
here, $d_i = p_{1_i} - p_{2_i}, \bar d = \frac{1} {n} \sum_{i=1}^n d_i, s_d = (\frac{1}{n-1}\sum_{i=1}^{n}(d_i - \bar d)^2)^{1/2}$  
In the significance level $\alpha$, the reject region is $$W = \{|T| \geq t_{1-\alpha /2}(n-1)\}$$.

## Homework 6
### 7.6 Question:

Efron and Tibshirani discuss the scor (bootstrap) test score data on 88 students who took examinations in five subjects [84, Table 7.1], [188, Table 1.2.1]. The first two tests (mechanics, vectors) were closed book and the last three tests (algebra, analysis, statistics) were open book. Each row of the data frame is a set of scores ($x_{i1}, . . . , x{i5}$) for the $i^{th}$ student. Use a panel display to display the scatter plots for each pair of test scores. Compare the plot with the sample correlation matrix. Obtain bootstrap estimates of the standard errors for each of the following estimates: $\hat ρ_{12} = \hat ρ_{12}$(mec, vec), $\hat ρ_{34} = \hat ρ_{34}$(alg, ana), $\hat ρ_{35} = \hat ρ_{35}$(alg, sta), $\hat ρ_{45} = \hat ρ_{45}$(ana, sta).
### Answer:
```{r}
library("bootstrap")
library("boot")

data(scor)
data <- as.matrix(scor)
color <- c("red", "blue", "yellow", "pink", "green")



plot(data[ ,1], col = color[1], pch = 20, ylim = c(0,100),
     xlab = "student", ylab = "score", main = "scatter plots")
for (i in 2:5) {     # draws the scatter plots.
  points(data[ ,i], col = color[i], pch = 20)
}
legend(80,100, c("mec","vec","alg","ana","sta"),
       col = color, pch =20)



subject <- data.frame(scor)
cor.est <- cor(subject, subject)

r <- function(x, i){    # computers the correlation coefficient.
  cor(x[i, 1], x[i,2])
}
r12 <- boot(data = data[ ,1:2], statistic = r, R = 2000)
r34 <- boot(data = data[ ,3:4], statistic = r, R = 2000)
r35 <- boot(data = data[ ,c(3,5)], statistic = r, R = 2000)
r45 <- boot(data = data[ ,4:5], statistic = r, R = 2000)


knitr::kable(round(cor.est, 4))


se.hat <- t(c(sd(r12$t), sd(r34$t), sd(r35$t), sd(r45$t)))
colnames(se.hat) <- c("r12se.hat", "r34se.hat", "r35se.hat", "r45se.hat")
knitr::kable(se.hat, digits = 4)
```

### 7.B Question:

Conduct a Monte Carlo study to estimate the coverage probabilities of the standard normal bootstrap confidence interval, the basic bootstrap confidence interval, and the percentile confidence interval. Sample from a normal population and check the empirical coverage rates for the sample skewness statistic. Find the proportion of times that the confidence intervals miss on the left, and the porportion of times that the confidence intervals miss on the right. Compare the coverage rates for normal populations (skewness 0) and $χ^2(5)$ distributions (positive skewness).
### Answer:
```{r}
library("boot")


skew <- function(x, i) {
  # caculates the sample skewness coefficient.
  y <- mean(x[i])
  return( mean((x[i] - y)^3) / mean((x[i] - y)^2)^1.5 )
}



cr <- function(distribution, alpha, r){
  # this function to compute the coverage rates of standard normal
  # bootstrap confidence interval (SNBCI), basic bootstrap confidence
  # interval (BBCI), and percentile confidence interval (PCI) for 
  # sample skewness statistic.
  
  # alpha is the confidence level
  # r is the times of bootstrap resampling.
  
  SNBCI.ucl <- SNBCI.lcl <- BBCI.ucl <- 
  BBCI.lcl <- PCI.ucl <- PCI.lcl <- numeric(r)
  
  for (i in 1:r) {
    if (distribution == "normal")     {x = rnorm(100)}
    else if (distribution == "chisq") {x = rchisq(100, df = 5)}
    
    # SB is the estimated statistic of the bootstrap resampling
    # here, we define the three interval's confidence limits.
    
    SB <- boot(data = x, statistic = skew, R=1000)
    SNBCI.ucl[i] <- SB$t0 - qnorm(alpha/2)*sd(SB$t)
    SNBCI.lcl[i] <- SB$t0 + qnorm(alpha/2)*sd(SB$t)
    BBCI.ucl[i] <- 2*SB$t0 - as.numeric(quantile(SB$t, alpha/2))
    BBCI.lcl[i] <- 2*SB$t0 - as.numeric(quantile(SB$t, 1-alpha/2))
    PCI.ucl[i] <- as.numeric(quantile(SB$t, 1-alpha/2)) 
    PCI.lcl[i] <- as.numeric(quantile(SB$t, alpha/2))
  }
  
  
  
  if (distribution == "normal")     {sktrue = 0}
  else if (distribution == "chisq") {sktrue = sqrt(8/5)}
  l1 <- mean( SNBCI.lcl >= sktrue )
  r1 <- mean( SNBCI.ucl <= sktrue )
  l2 <- mean( BBCI.lcl  >= sktrue )
  r2 <- mean( BBCI.ucl  <= sktrue )
  l3 <- mean( PCI.lcl   >= sktrue )
  r3 <- mean( PCI.ucl   <= sktrue )
  cr1 <- 1 - l1 - r1
  cr2 <- 1 - l2 - r2
  cr3 <- 1 - l3 - r3
  list(c(SNBCI.cr = cr1, SNBCI.leftmiss = l1,
         SNBCI.rightmiss = r1),
       c(BBCI.cr = cr2, BBCI.leftmiss = l2,
         BBCI.rightmiss = r2),
       c(PCI.cr = cr3, PCI.leftmiss = l3,
         PCI.rightmiss = r3))
}
normal <- cr("normal", .05, 1000)
knitr::kable(rbind(unlist(normal)))


chisq <- cr("chisq", .05, 1000)
knitr::kable(rbind(unlist(chisq)))


```

> For the same sample data, the standard normal bootstrap C.I. has a better coverage probabilties. Compare the normal population and chisq population, we can see the coverage rates of normal population is bigger than chisq population.

## Homework 7
### 7.8 Question:

Efron and Tibshirani discuss the scor (bootstrap) test score data on 88 studentswho took  examinations in five subjects. The five-dimensional scores data have a 5 × 5 covariance matrix $Σ$, with positive eigenvalues $λ_1 > · · · > λ_5$. In principal components analysis, $$θ = \frac{λ_1}{\sum^5_{j=1}\lambda_j}$$ measures the proportion of variance explained by the first principal component. Let $\hatλ_1 > · · · > \hatλ_5$ be the eigenvalues of $\hatΣ$, where $\hatΣ$ is the MLE of $Σ$. Compute the sample estimate $$\hatθ = \frac{\hatλ_1}{\sum^5_{j=1}\hat\lambda_j}$$of $θ$. Use jackknife to estimate the bias and standard error of $\hatθ$.  
### Answer:
```{r}
library(bootstrap)
m <- data.matrix(scor)
n <- nrow(m)

lambda.hat <- eigen(t(m)%*%m/n)$values
theta.hat <- lambda.hat[1] / sum(lambda.hat)

### jackknife
theta.j <- numeric(n)
for (i in 1:n) { # computes the estimator of theta by jackknife
  m.j <- m[-i,]
  lambda.j <- eigen(t(m.j)%*%m.j/(n-1))$values
  theta.j[i] <- lambda.j[1] / sum(lambda.j)
}
bias.est <- (n-1) * ( mean(theta.j)-theta.hat )
se.est <- sqrt( ((n-1)/n) * sum((theta.j-mean(theta.j))^2) )
print(c(bias.est = bias.est, se.est = se.est))
```
### 7.10 Question:

In Example 7.18, leave-one-out (n-fold) cross validation was used to select the best fitting model. Repeat the analysis replacing the Log-Log model with a cubic polynomial model. Which of the four models is selected by the cross validation procedure? Which model is selected according to maximum adjusted $R^2$?  
### Answer:
```{r}
library(DAAG); attach(ironslag)

n <- length(magnetic)
e1 <- e2 <- e3 <- e4 <- numeric(n)
# for n-fold cross validation
# fit models on leave-one-out samples
for (k in 1:n) {
  y <- magnetic[-k]
  x <- chemical[-k]
  
  z1 <- lm(y ~ x)      # linear model
  yhat1 <- z1$coef[1] + z1$coef[2] * chemical[k]
  e1[k] <- magnetic[k] - yhat1
  
  z2 <- lm(y ~ x + I(x^2)) # quadratic model
  yhat2 <- z2$coef[1] + z2$coef[2] * chemical[k] +
    z2$coef[3] * chemical[k]^2
  e2[k] <- magnetic[k] - yhat2
  
  z3 <- lm(log(y) ~ x)    # exponential model
  logyhat3 <- z3$coef[1] + z3$coef[2] * chemical[k]
  yhat3 <- exp(logyhat3)
  e3[k] <- magnetic[k] - yhat3
  
  z4 <- lm(y ~ x + I(x^2) + I(x^3))
                          # cubic polynomial model
  yhat4 <- z4$coef[1] + z4$coef[2] * chemical[k] +
    z4$coef[3] * chemical[k]^2 + z4$coef[4] * chemical[k]^3
  e4[k] <- magnetic[k] - yhat4
}

round(c(mean(e1^2), mean(e2^2), mean(e3^2), mean(e4^2)), 4)

round(c(summary(z1)$adj.r.squared, summary(z2)$adj.r.squared,
  summary(z3)$adj.r.squared, summary(z4)$adj.r.squared), 4)
```
> According to prediction error criterion, the quadratic polynomial model is still the best fit. While the cubic polynomial model is the best fit model based on maximum adjusted R square. 


## Homework 8
### 8.3 Question:

The Count 5 test for equal variances in Section 6.4 is based on the maximum number of extreme points. Example 6.15 shows that the Count 5 criterion is not applicable for unequal sample sizes. Implement a permutation test for equal variance based on the maximum number of extreme points that applies when sample sizes are not necessarily equal.

### Answer:
```{r}
## p.test is a permutation test for the different size
## we suppose x, y are drawn from normal distribution,
## and with the same mean, but different variances and
## different sample sizes

p.test <- function(n1, n2, mu1 ,mu2 ,sigma1, sigma2) {
  x <- rnorm(n1, mu1, sigma1)
  y <- rnorm(n2, mu2, sigma2)    # get the normal sample.
  count <- function(x, y) {      # count the number 
                                 # of extreme points.
    
    X <- x - mean(x)
    Y <- y - mean(y)             # centralization
    countx <- sum(X > max(Y)) + sum(X < min(Y))
    county <- sum(Y > max(X)) + sum(Y < min(X))
    return( max(c(countx,county)) )
  }
  
  
  T.hat <- count(x, y)           # this is the statistic for
                                 # the original sample.
  
  
  r <- 99                      # resampling times.
  T <- numeric(r)
  z <- c(x, y)
  
  for (i in 1:r) {               # compute the statistic 
                                 # for r times resampling.
    k <- sample(1:(n1 + n2), size = n1, replace = FALSE)
    x <- z[k]
    y <- z[-k]
    T[i] <- count(x, y)
  }
  
  mean(c(T.hat, T) >= T.hat)
}



set.seed(1)
p1 <- p.test(50, 70, 0, 0, 1, 1)
p2 <- p.test(50, 70, 0, 0, 1, 2)
print(c(equal.asl = p1, diff.asl = p2))
```
> By the results, we can find the asl is bigger than 0.5 when two samples with equal variances, so we can not reject the null hypothesis.

### Additional exercise

Suppose:  
model1: $Y = X / 4 + e$  
model2: $Y = X / 4 \times e$  
$X \sim N(0_2, I_2), e \sim N(0_2, I_2)$, X and e are independent.

Please compare their power: distance correlation test versus ball covariance test

### Answer:
```{r}
library(MASS)
library(Ball)
library(energy)


n <- seq(20, 100, l=5)         # sample size
mu <- rep(0, 2)                # this is the mean
sigma <- diag(2)               # this is the variance
alpha <- .1                    # the significance level


p1 <- p2 <- numeric(length(n))

## here, we computer the model 1 firstly.
for (i in 1:length(n)) {
  
  pv1 <- replicate(100, expr = {      # get pvalue
    x <- mvrnorm(n[i], mu, sigma)     # sampling x
    e <- mvrnorm(n[i], mu, sigma)     # sampling e
    y <- x / 4 + e
    dcov.test(x, y, R = 99)$p.value})
  
  p1[i] <- mean( pv1 < alpha )        # this is the power
  
  
  
  pv2 <- replicate(100, expr = {
    x <- mvrnorm(n[i], mu, sigma)     # sampling x
    e <- mvrnorm(n[i], mu, sigma)     # sampling e
    y <- x / 4 + e
    as.numeric(bcov.test(x, y, R = 99, seed = NULL)$p.value)})
                     
  p2[i] <- mean( pv2 < alpha)
}


plot(n, p1, type = "b", xlab = "n", ylab = "power",
     main = expression(Y == X/4 + e), col = "blue", lty = 3)
lines(n, p2, type = "b", col = "red", lty = 3)
legend(5, 1, paste(c("dcov","Ball")), col = c("blue","red"),
       lty = c(3,3), cex = .7)




## then, repeat the process to computer the model 2

n <- seq(10, 210, l=5)
p_1 <- p_2 <- numeric(length(n))

for (i in 1:length(n)) {
  
  pv_1 <- replicate(100, expr = {
    x <- mvrnorm(n[i], mu, sigma)     # sampling x
    e <- mvrnorm(n[i], mu, sigma)     # sampling e
    y <- (x / 4) * e
    dcov.test(x, y, R = 99)$p.value})
  
  p_1[i] <- mean( pv_1 < alpha )
  
  
  pv_2 <- replicate(100, expr = {
    x <- mvrnorm(n[i], mu, sigma)     # sampling x
    e <- mvrnorm(n[i], mu, sigma)     # sampling e
    y <- (x / 4) * e
    as.numeric(bcov.test(x, y, R = 99, seed = NULL)$p.value)})
  
  p_2[i] <- mean( pv_2 < alpha)
}

plot(n, p_1, type = "b", xlab = "n", ylab = "power",
     main = expression(Y == X/4 %*% e), col = "blue", lty = 3)
lines(n, p_2, type = "b", col = "red", lty = 3)
legend(150, .5, paste(c("dcov","Ball")), col = c("blue","red"),
       lty = c(3,3), cex = 1)
```

> These pictures shows that dcov.test is powerful than bcov.test in model1, which is opposite in the model2.


## Homework 9
### 9.4 Question:

Implement a random walk Metropolis sampler for generating the standard Laplace distribution. For the increment, simulate from a normal distribution. Compare the chains generated when different variances are used for the proposal distribution. Also, compute the acceptance rates of each chain.

### Answer:
```{r}
set.seed(1)

df <- function(x) {    # df is the density function
                       # of standard laplace distribution.
  f <- exp(-abs(x)) / 2
  return(f)
}

quan <- function(q) {  # quan is the func to find the quantile
                       # of standard laplace distribution.
  
  df <- function(x) {
    f <- exp(-abs(x)) / 2
    return(f)
  }
  
  cumprob <- function(y) {
    f <- integrate(df, -Inf, y)$value - q
    return(f)
  }
  uniroot(cumprob, c(-10, 10))$root
}




rw.Metropolis <- function(sigma, x0, N) {
  x <- numeric(N)
  x[1] <- x0
  u <- runif(N)
  k <- 0
  for (i in 2:N) {
    y <- rnorm(1, x[i-1], sigma)
    if (u[i] <= (df(y) / df(x[i-1])))
      x[i] <- y  
    else {
      x[i] <- x[i-1]
      k <- k + 1
    }
  }
  return(list(x=x, k=k))
}





N <- 2000
sigma <- c(.05, .5, 2, 16)
x0 <- 20

rw1 <- rw.Metropolis(sigma[1], x0, N)
rw2 <- rw.Metropolis(sigma[2], x0, N)
rw3 <- rw.Metropolis(sigma[3], x0, N)
rw4 <- rw.Metropolis(sigma[4], x0, N)

rw <- cbind(rw1$x, rw2$x, rw3$x,  rw4$x)
for (j in 1:4) {
  plot(rw[,j], type="l",
       xlab=bquote(sigma == .(round(sigma[j],3))),
       ylab="X", ylim=range(rw[,j]))
  abline(h = c(quan(.025), quan(.975)))
}

a <- c(.05, seq(.1, .9, .1), .95)

Q <- numeric(length(a))
for (i in 1:length(a)) {
  Q[i] <- quan(a[i])
}

mc <- rw[501:N, ]

Qrw <- apply(mc, 2, function(x) quantile(x, a))
qq <- data.frame(round(cbind(Q, Qrw), 3))
names(qq) <- c('True','sigma=0.05',
               'sigma=0.5','sigma=2','sigma=16')

knitr::kable(qq)

rate <- (N - c(rw1$k, rw2$k, rw3$k, rw4$k)) / N
acceptance.rate <- data.frame(sigma = sigma, 
                              acceptance.rate = rate)
knitr::kable(acceptance.rate)

```


## Homework 10
### 11.1 Question:

The natural logarithm and exponential functions are inverses of each other, so that mathematically log(exp $x$) = exp(log $x$) = $x$. Show by example that this property does not hold exactly in computer arithmetic. Does the identity hold with near equality? (See all.equal.)

### Answer:
```{r}
x <- 0.1
ex1 <- exp(log(x))
ex2 <- log(exp(x))
isTRUE(ex1 == ex2)
isTRUE(all.equal(ex1, ex2))
```

>This shows that the two expressions are not equal exactly in computer arithmetic, while they are near equal.





### 11.5 Question:

Write a function to solve the equation
$$
\begin{aligned} \frac{2 \Gamma\left(\frac{k}{2}\right)}{\sqrt{\pi(k-1)}\Gamma\left(\frac{k-1}{2}\right)} \int_{0}^{c_{k-1}}\left(1+\frac{u^{2}}{k-1}\right)^{-k / 2} \mathrm d u & 
\\= \frac{2 \Gamma\left(\frac{k+1}{2}\right)}{\sqrt{\pi k}\Gamma\left(\frac{k}{2}\right)}\int_{0}^{c_{k}}\left(1+\frac{u^{2}}{k}\right)^{-(k+1) / 2} \mathrm d u \end{aligned}
$$
for $a$, where
$$
c_{k}=\sqrt{\frac{a^{2} k}{k+1-a^{2}}}
$$ 
Compare the solutions with the points $A(k)$ in Exercise 11.4.


### Answer:

recall ex 11.4:
```{r}
b <- c(4:25, 100, 500, 1000) # this is the freedom degree.
m <- matrix(NA, nrow = 25, ncol = 3)

f <- function(a, k) {
  pt(sqrt(a^2*k/(k+1-a^2)), df = k, log.p = TRUE) -
    pt(sqrt(a^2*(k-1)/(k-a^2)), df = k-1, log.p = TRUE)}


for (i in 1:25) {
  k <- b[i]
  xx <- seq(1e-5, sqrt(b[i])-1e-5, .01)
  g <- function(a) f(a, b[i])
  yy <- apply(as.matrix(xx), 1, g)
  plot(xx, yy, type = "l", main = paste("k=",b[i]),
       xlab = "a", ylab = "f")
  abline(h = 0, col = "red",lty=3)
}
```

By these pictures, there is only one intersection point in $(0,2)$. so we can use 'uniroot' straightly to find the intersection point in $(0,2)$ interval. when $a >2$ , it also shows that the two value of function is very close, but we guess they are just very near , other than equal. so we abandon the solution in $(2, \sqrt k )$.

then , give the solution of 11.5

```{r}
ak1 <- function(k) {
 # to computer the intersection point in exercise 11.4
  res <- uniroot(f, c(1e-5, 2), k=k)
  res$root
}

m[ ,1] <- apply(as.matrix(b), 1, ak1)



F <- function(a, k) {
  # this function is to calculate s(a, k) - s(a, k+1)
  ck <- function(a, k) sqrt( a^2*k/(k+1-a^2))
  g <- function(u) (1+u^2/(k-1))^(-k/2)
  int <- function(a, k) integrate(g, lower = 0, upper = ck(a, k))
  
  s <- function(a, k) {
    exp( log(2/sqrt(pi*(k-1))) + lgamma(k/2)
         - lgamma((k-1)/2) +log(int(a,k)$value))
  }
  
  s(a, k) - s(a, k+1)
}


for (i in 1:25) {
  k <- b[i]
  xx <- seq(1e-5, sqrt(b[i])-1e-5, 0.1)
  G <- function(a) F(a, b[i])
  yy <- apply(as.matrix(xx), 1, G)
   plot(xx, yy, type = "l", main = paste("k=",b[i]),
       xlab = "a", ylab = "F")
  abline(h = 0, col = "red")
}
```

Here, there are two intersection points in $(0,\sqrt{k})$ obviously, since the curve shows the trend, we can separete the $(0,\sqrt{k})$ into in $(0,2)$ , $(2,\sqrt{k})$.

```{r}
ak2 <- function(k) {
  # to computer the intersection point in (0,2)
  res <- uniroot(F, c(1e-5, 2), k=k)
  res$root
}
m[ ,2] <- apply(as.matrix(b), 1, ak2)

ak3 <- function(k) {
  # to computer the intersection point
  res <- uniroot(F, c(1.8, 2.5), k=k)
  res$root
}

m[3:25,3] <- apply(as.matrix(b[3:25]), 1, ak3)
m[2,3] <- uniroot(F, c(1.8, sqrt(5)), k=5)$root
colnames(m) <- c("intersection point in 11.4", 
                 "1st intersection point in 11.5",
                 "2nd intersection point in 11.5")

knitr::kable(m)
```





### EXERCISE: A-B-O blood type problem

Let the three alleles be $A, B,$ and $O$ with allele frequencies $p,$ $q,$ and $r .$ The 6 genotype frequencies under HWE and complete counts are as follows.
$$
\begin{array}{|c|c|c|c|c|c|c|}\hline \text { Genotype } & {A A} & {B B} & {O O} & {A O} & {B O} & {A B} & {S u m} \\ \hline \text { Frequency } & {p^{2}} & {q^{2}} & {r^{2}} & {2 p r} & {2 q r} & {2 p q} & {1} \\ \hline \text { Count } & {n A A} & {n B B} & {n O O} & {n A O} & {n B O} & {n A B} & {n} \\ \hline\end{array}
$$
Observed data: 

$n_{A}=n_{A A}+n_{A O}=28(\text { A - type }),$

$n_{B .}=n_{B B}+n_{B O}=24(\text {B - type }),$

$n_{O O}=41(\text {O - type }),$

$n_{A B}=70(\text {AB - type })$

question1: Use EM algorithm to solve MLE of $p$ and $q$ (consider missing data $n_{AA}$ and $n_{BB}$).

question2: Show that the log-maximum likelihood values in M-steps are increasing via line plot.

### Answer:
For this problem, we can give the expectation of log likelihood function:
$$
\begin{aligned} E \log f\left(y | \lambda^{(t)}\right) &=n_{A A}^{(t)} \cdot \log p^{2}+n_{B B}^{(t)} \cdot \log q^{2}+n_{OO}^{(t)} \cdot \log r^{2} \\ &+n_{A B}^{(t)} \cdot \log (2 p q)+n_{A O}^{(t)} \cdot \log (2 p r)+n_{B O}^{(t)} \cdot \log (2 q r) 
\\ &+ E \cdot log \frac{n !}{n_{A A}^{(t)}! n_{A B}^{(t)} ! n_{B B}^{(t)} ! n_{OO}^{(t)} ! n_{AO}^{(t)}! n_{BO}^{(t)}!}
\end{aligned}
$$
here, in the $t$ step:
$$
\begin{array}{l}{n_{A A}^{(t)}=\left(n_{A} \cdot\right) \frac{\left(p^{(t)}\right)^{2}}{\left(p^{(t)}\right)^{2}+2 p^{(t)} \cdot r^{(t)}}}
\\ {n_{B B}^{(t)}=\left(n_{B} \cdot\right) \frac{\left(q^{(t)}\right)^{2}}{\left(q^{(t)}\right)^{2}+2 q^{(t)} \cdot r^{(t)}}} 
\\ {n_{A O}^{(t)}=\left(n_{A} \cdot\right) \frac{2 p^{(t)} \cdot r^{(t)}}{\left(p^{(t)}\right)^{2}+2 p^{(t)} r^{(t)}}} 
\\ {n_{B O}^{(t)}=\left(n_{B}\cdot\right) \frac{2 q^{(t)} \cdot r^{(t)}}{\left(q^{(t)}\right)^{2}+2 q^{(t)} \cdot r^{(t)}}}\end{array}
$$
and by maximum the log likelihood , we can get the $p, q, r$ in $t + 1$step.
$$
\begin{array}{l}{p^{(t+1)}=\frac{2 n_{A A}^{(t)}+n_{A B}^{(t)}+n_{A O}^{(t)}}{2 n}} \\ {q^{(t+1)}=\frac{2 n_{B B}^{(t)}+n_{A B}^{(t)}+n_{B O}^{(t)}}{2 n}} \\ {r^{(t+1)}=\frac{2 n_{OO}^{(t)}+n_{A O}^{(t)}+n_{BO}^{(t)}}{2 n}}\end{array}
$$

```{r}
N <-100
tol <- .Machine$double.eps^0.5   # the digit of converge
na <- 24
nb <- 28
noo <- 41
nab <- 70
n <- sum(na, nb, noo, nab)

p <- q <- r <- Elogl <- rep(0, N)
p[1] <- q[1] <- 1/3              # the original p, q ,r
k <- 0
for (i in 2:N) {
  p.old <- p[i-1]
  q.old <- q[i-1]
  r.old <- 1-p.old-q.old
  
  naa <- na * p.old^2 / (p.old^2 + 2*p.old*r.old)
  nbb <- nb * q.old^2 / (q.old^2 + 2*q.old*r.old)
  nao <- na * 2 * p.old * r.old / (p.old^2 + 2*p.old*r.old)
  nbo <- nb * 2 * q.old * r.old / (q.old^2 + 2*q.old*r.old)
  
  p[i] <- sum(2*naa, nab, nao) / (2*n)
  q[i] <- sum(2*nbb, nab, nbo) / (2*n)
  r[i] <- sum(2*noo, nao, nbo) / (2*n)
  
  if (sum( abs(p[i]-p.old)/p.old < tol))
      break
  
  Elogl[i-1] <- naa*2*log(p[i]) + nbb*2*log(q[i]) + 
    noo*2*log(r[i]) + nab*log(2*p[i]*q[i]) +
    nao*log(2*p[i]*r[i]) +
    nbo*log(2*q[i]*r[i])
  prob.hat <- c(p[i], q[i], r[i])
  k <- k + 1
}

print(round(c(p.hat=prob.hat[1], q.hat=prob.hat[2]),3))
plot(1:k, Elogl[1:k], type = "b",
     xlab = "the recursive times", ylab = "Elogl")
```


## Homework 11
### 11.1.3 Question:

Use both for loops and lapply() to fit linear models to the mtcars using the formulas stored in this list:

formulas <- list(

mpg ~ disp,

mpg ~ I(1 / disp),

mpg ~ disp + wt,

mpg ~ I(1 / disp) + wt

)

### Answer:
```{r}
disp <- mtcars$disp
mpg <- mtcars$mpg
wt <- mtcars$wt


formulas <- list(
  model1 = mpg ~ disp,
  model2 = mpg ~ I(1 / disp),
  model3 = mpg ~ disp + wt,
  model4 = mpg ~ I(1 / disp) + wt
  )

# by using the lapply function.
lapply(formulas, function(x) lm(x)$coefficients)

# by using the for loop.
# we creat a list to save the results firstly.

forloops <- coefficient <- list(model1="1", model2="2",
                            model3="3", model4="4")
for (i in 1:4) {
  forloops[[i]] <- lm(formulas[[i]])
  coefficient[[i]] <- forloops[[i]]$coefficients
}
coefficient
```


### 11.1.4 Question:
Fit the model mpg ~ disp to each of the bootstrap replicates of mtcars in the list below by using a for loop and lapply(). Can you do it without an anonymous function?

bootstraps <- lapply(1:10, function(i) {

rows <- sample(1:nrow(mtcars), rep = TRUE)

mtcars[rows, ]

})

### Answer:
```{r}

bootstraps <- lapply(1:10, function(i) {
  rows <- sample(1:nrow(mtcars), rep = TRUE)
  mtcars[rows, ]
})


# by using lapply.
temp <- lapply(bootstraps, function(x) lm(x$mpg ~ x$disp)$coefficients)
m1 <- vapply(temp, as.numeric, matrix(0, nrow = 2))
colnames(m1) <- "fit"; rownames(m1) <- c("intercept", "slope")
knitr::kable(round(m1, 4))


# by using a for loop.
forloop <- coefficient2 <- list(fit1="1", fit2="2", fit3="3",
                                fit4="4", fit5="5",fit6="6", 
                                fit7="7",fit8="8",fit9="9", fit10="10")
for (i in 1:10) {
  x <- bootstraps[[i]]
  forloop[[i]] <- lm(x$mpg ~ x$disp)
  coefficient2[[i]] <- forloop[[i]]$coefficients
}
temp <- coefficient2
m2 <- vapply(temp, as.numeric, matrix(0, nrow = 2))
colnames(m2) <- "model"; rownames(m2) <- c("intercept", "slope")
knitr::kable(round(m2, 4))

# without using an anonymous function.
fitfunc <- function(n){
  resmodel <- rescoefficient <- forloop
  for (i in 1:n) {
    rows <- sample(1:nrow(mtcars), rep = TRUE)
    data <- mtcars[rows, ]
    resmodel[[i]] <- lm(data$mpg ~ data$disp)
    rescoefficient[[i]] <- resmodel[[i]]$coefficients
  }
  list(model = resmodel, coefficient = rescoefficient)
}

res <- fitfunc(10)
temp <- res$coefficient
m3 <- vapply(temp, as.numeric, matrix(0, nrow = 2))
colnames(m3) <- "model"; rownames(m3) <- c("intercept", "slope")
knitr::kable(round(m3, 4))
```

### 11.1.5 Question:

For each model in the previous two exercises, extract R2 using the function below.
rsq <- function(mod) summary(mod)$r.squared

### Answer:

```{r}
## for exercise 11.1.3
rsq <- function(mod) summary(mod)$r.squared
r1 <- unlist(lapply(forloops, rsq))
knitr::kable(round(t(r1), 4))

## for exercise 11.1.4 with using anonymous function.
r2 <- unlist(lapply(forloop,rsq))
knitr::kable(round(t(r2), 4))

## for exercise 11.1.4 without anonymous function.
r3 <- unlist(lapply(res$model, rsq))
knitr::kable(round(t(r3), 4))
```


### 11.2.3 Question:

The following code simulates the performance of a t-test for non-normal data. Use sapply() and an anonymous function to extract the p-value from every trial. 

trials <- replicate(

100,

t.test(rpois(10, 10), rpois(7, 10)),

simplify = FALSE

)

Extra challenge: get rid of the anonymous function by using
[[ directly.

### Answer:

```{r}
set.seed(1)
trials <- replicate(
  100,
  t.test(rpois(10, 10), rpois(7, 10)),
  simplify = FALSE
)

# by using sapply and anonymous function.
sapply(trials, function(x) x$p.value)

# by using sapply and '[[' directly.
sapply(trials, '[[', 3)
```

### 11.2.7 Question:

Implement mcsapply(), a multicore version of sapply(). Can you implement mcvapply(), a parallel version of vapply()? Why or why not?

### Answer:
```{r}
## recall the additional homework in 11/22, the solution is really 
## taking much time. For this reason, we attempt to write a code 
## with parallel computing by parSapply function. We suppose five
## sampling size (5,10,15,20,25) for the model Y = X/4+e, computing
## the p-value by sapply and parSapply respectively, and compare 
## with the results, by the way, it also contains the consuming 
## time for the consequence.


library(parallel)
library(MASS)
library(energy)

p.value <- function(n) {
  mu <- rep(0, 2)
  sigma <- diag(2) 
  alpha <- .1 
  
  pv1 <- replicate(1000, expr = {
    x <- MASS::mvrnorm(n, mu, sigma)
    e <- MASS::mvrnorm(n, mu, sigma)
    y <- x / 4 + e
    energy::dcov.test(x, y, R = 199)$p.value})
  
  p <- mean( pv1 < alpha )
  return(p)
}

n <- seq(5, 25, 5)
sapply(n, p.value)
cl <- makeCluster(4)
parSapply(cl, n, p.value)
system.time(sapply(n, p.value))
system.time(parSapply(cl, n, p.value))
stopCluster(cl)
```

As for the parallel verison of vapply. I think it is infeasible. If we use the
mcvapply, you must assign the your output type in mcvapply. However, since the parallel computing operates by dividing into different cores. The length of vector in single core is possiblely different to the length of combing results which you assign, it will generate an error.





## Homework 12
### Question:
You have already written an R function for Exercise 9.4 (page277, Statistical Computing with R). Rewrite an Rcpp function for the same task.

1.Compare the generated random numbers by the two functions
using qqplot.

2.Campare the computation time of the two functions with
microbenchmark.

3.Comments your results.

### Answer:
```{r}
library(Rcpp)
library(microbenchmark)
set.seed(1)

# df is the density function
# of standard laplace distribution.
df <- function(x) {    
  f <- exp(-abs(x)) / 2
  return(f)
}

rwmR<- function(sigma, x0, N) {
  x <- numeric(N)
  x[1] <- x0
  u <- runif(N)
  for (i in 2:N) {
    y <- rnorm(1, x[i-1], sigma)
    if (u[i] <= (df(y) / df(x[i-1])))
      x[i] <- y  
    else {
      x[i] <- x[i-1]
    }
  }
  return(x)
}


cppFunction(
'NumericVector rwmC(double sigma, double x0, int N){
NumericVector x(N);
x[0] = x0;
NumericVector u = as<NumericVector>(runif(N));

for(int i = 1; i < N; i++){
double y = as<double>(rnorm(1, x[i-1], sigma));
if (u[i] <= (exp(-abs(y))/2) / (exp(-abs(x[i-1])) / 2)){
x[i] = y;
}else{
x[i] = x[i-1];
}
}
return(x);
}')


N <- 2000
x0 <- 20
sigma <- c(.05, .5, 2, 16)

rwmR1 <- rwmR(sigma[1], x0, N)
rwmR2 <- rwmR(sigma[2], x0, N)
rwmR3 <- rwmR(sigma[3], x0, N)
rwmR4 <- rwmR(sigma[4], x0, N)

rwmC1 <- rwmC(sigma[1], x0, N)
rwmC2 <- rwmC(sigma[2], x0, N)
rwmC3 <- rwmC(sigma[3], x0, N)
rwmC4 <- rwmC(sigma[4], x0, N)

rwmR <- cbind(rwmR1, rwmR2, rwmR3,  rwmR4)
rwmC <- cbind(rwmC1, rwmC2, rwmC3,  rwmC4)


for (j in 1:4) {
  par(mfrow=c(1,2))
  qqnorm(rwmR[ ,j])
  qqline(rwmR[ ,j])
  qqnorm(rwmC[ ,j])
  qqline(rwmC[ ,j])
}

dir <- 'C:/'
source(paste0(dir, 'rwmR.R'))
sourceCpp(paste0(dir, 'rwmC.cpp'))
microbenchmark(rwmR(2, x0, N), rwmC(2, x0, N))
```

By the QQ-plot, we can see the computing results are nearly same between rwmC and rwmR. But considering the computeing times, rwmC is better than rwmR obviously.
